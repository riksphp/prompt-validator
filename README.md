# Prompt Validator

A Chrome Extension and Web App with **sequential AI-powered analysis** - where an LLM router intelligently decides each processing step.

## ğŸ¯ Key Features

- ğŸ§  **Sequential LLM Router**: AI decides the next action step-by-step
- âœ… **Prompt Validation**: Analyze prompts for quality and structure
- ğŸ‘¤ **Context Extraction**: Personal, professional, task, and external context
- âœ¨ **Intelligent Improvement**: Generate enhanced prompts using accumulated context
- ğŸ”„ **Iterative Processing**: Router called repeatedly until complete
- ğŸ“Š **Real-Time Progress**: See each decision and action as it happens
- ğŸ’¾ **Persistent Storage**: All context and prompts saved automatically to localStorage
- ğŸ“œ **Prompt History**: View, search, and reuse past prompts with full analysis
- ğŸ¨ **Modern UI**: Dark glassmorphic design with live step visualization
- ğŸ”§ **Dual Mode**: Works as Chrome Extension AND standalone web app

## ğŸ¯ Router Quality - All Checks Passing!

The router prompt has been validated and **passes all quality checks**:

```
âœ… Explicit Reasoning          âœ… Internal Self-Checks
âœ… Structured Output           âœ… Reasoning Type Awareness
âœ… Tool Separation             âœ… Fallbacks
âœ… Conversation Loop           âœ¨ Production Ready
âœ… Instructional Framing
```

**Key Features:**

- ğŸ§  **Reasoning Type Awareness**: LLM declares its reasoning approach (analytical, sequential, pattern-matching, contextual)
- ğŸ” **Pattern Matching**: Prevents duplicate actions by recognizing similar completed actions
- ğŸ›¡ï¸ **Edge Case Handling**: Considers potential issues before making decisions
- ğŸ“Š **Confidence Calibration**: Honest confidence scores (0.0-1.0) trigger fallbacks when needed
- âš ï¸ **Anti-Hallucination**: Warns against making up data, extracts only what's present

See [ROUTER_PROMPT_ENHANCEMENT.md](./ROUTER_PROMPT_ENHANCEMENT.md) for full details.

---

## ğŸ§  Sequential Router Architecture

### **How It Works**

Instead of deciding everything upfront, the system:

1. **Calls `routePrompt()`** â†’ LLM decides the first action
2. **Executes that action** (e.g., validate prompt)
3. **Calls `routePrompt()` again** â†’ LLM decides the next action (knowing what's been done)
4. **Executes next action** (e.g., extract professional info)
5. **Repeats until complete** â†’ LLM returns "done"

### **Example Flow**

```
User Input: "I'm a React developer building an auth system"

Step 1: routePrompt() â†’ "validate" â†’ âœ… Validates prompt quality
Step 2: routePrompt() â†’ "extractProfessional" â†’ ğŸ’¼ Extracts React/developer info
Step 3: routePrompt() â†’ "extractTask" â†’ ğŸ“‹ Extracts auth system task
Step 4: routePrompt() â†’ "extractExternal" â†’ ğŸ”§ Extracts React framework
Step 5: routePrompt() â†’ "extractTags" â†’ ğŸ·ï¸ Generates tags
Step 6: routePrompt() â†’ "generateImprovement" â†’ âœ¨ Creates enhanced prompt
Step 7: routePrompt() â†’ "done" â†’ ğŸ‰ Complete!
```

## ğŸ“‹ Available Actions

The LLM router can choose from:

| Action                | Icon | Description                              |
| --------------------- | ---- | ---------------------------------------- |
| `validate`            | âœ…   | Validate prompt quality and structure    |
| `extractPersonal`     | ğŸ‘¤   | Extract name, location, goals, interests |
| `extractProfessional` | ğŸ’¼   | Extract job, tech stack, projects        |
| `extractTask`         | ğŸ“‹   | Extract current task being worked on     |
| `extractIntent`       | ğŸ¯   | Extract primary goal and intent          |
| `extractTone`         | ğŸ¨   | Extract style and tone preferences       |
| `extractExternal`     | ğŸ”§   | Extract tools, frameworks, APIs          |
| `extractTags`         | ğŸ·ï¸   | Generate relevant keywords               |
| `generateImprovement` | âœ¨   | Create improved prompt with context      |
| `done`                | ğŸ‰   | Signal completion                        |

## ğŸš€ Quick Start

### Prerequisites

- Node.js (v16 or higher)
- npm
- Google Gemini API key ([Get one here](https://makersuite.google.com/app/apikey))

### Installation

1. **Install dependencies**

   ```bash
   cd prompt-validator
   npm install
   ```

2. **Development (Web App)**

   ```bash
   npm run dev
   ```

   Open http://localhost:5173

3. **Build for production**

   ```bash
   npm run build
   ```

### Chrome Extension Setup

1. **Build the extension**

   ```bash
   npm run build
   ```

2. **Load in Chrome**

   - Open `chrome://extensions`
   - Enable "Developer mode"
   - Click "Load unpacked"
   - Select the `dist/` folder
   - Pin the extension to your toolbar

3. **Configure API Key**
   - Click the extension icon
   - Click "Settings" âš™ï¸
   - Select provider (Gemini, OpenAI, Groq, Custom)
   - Enter your API key
   - Save settings

## ğŸ—ï¸ Project Structure

```
prompt-validator/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ Home.tsx                    # Main UI with live steps
â”‚   â”‚   â”œâ”€â”€ Home.module.css             # Modern dark theme
â”‚   â”‚   â”œâ”€â”€ SettingsPage.tsx            # Comprehensive settings
â”‚   â”‚   â””â”€â”€ SettingsPage.module.css     # Settings styling
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ llmRouter.ts                # Sequential LLM router â­
â”‚   â”‚   â”œâ”€â”€ intelligentOrchestrator.ts  # Iterative orchestration â­
â”‚   â”‚   â”œâ”€â”€ llmClient.ts                # LLM API integration
â”‚   â”‚   â”œâ”€â”€ contextStorage.ts           # Context persistence
â”‚   â”‚   â””â”€â”€ aiSettingsStorage.ts        # Settings management
â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â”œâ”€â”€ index.ts                    # Core types
â”‚   â”‚   â”œâ”€â”€ context.ts                  # Context types
â”‚   â”‚   â””â”€â”€ chrome.d.ts                 # Chrome API types
â”‚   â”œâ”€â”€ main.tsx                        # App entry point
â”‚   â””â”€â”€ styles.css                      # Global styles
â”œâ”€â”€ public/
â”‚   â””â”€â”€ manifest.json                   # Chrome Extension manifest
â”œâ”€â”€ SEQUENTIAL_ROUTER.md                # ğŸ“– Router architecture docs
â”œâ”€â”€ ARCHITECTURE.md                     # ğŸ“– System architecture
â”œâ”€â”€ CONTEXT_FEATURE.md                  # ğŸ“– Context extraction docs
â””â”€â”€ README.md                           # This file
```

## ğŸ”§ Architecture

### Core Components

1. **LLM Router (`llmRouter.ts`)**

   - Makes sequential decisions
   - Sees what's already been done
   - Returns ONE next action
   - Provides reasoning for each decision

2. **Intelligent Orchestrator (`intelligentOrchestrator.ts`)**

   - Calls router iteratively
   - Executes each action
   - Tracks progress
   - Updates UI in real-time

3. **Context Storage (`contextStorage.ts`)**

   - Persists extracted information
   - Accumulates user context over time
   - Provides context summary for improvements

4. **UI Components (`Home.tsx`)**
   - Live step visualization
   - Router decision display
   - Real-time progress updates
   - Result presentation

### Tech Stack

- **Frontend**: React 18 + TypeScript
- **Build**: Vite
- **Styling**: Tailwind CSS + CSS Modules
- **AI**: Google Gemini API (or OpenAI, Groq, Custom)
- **Storage**: Chrome Storage API / localStorage

## ğŸ“– Usage

### Basic Flow

1. **Enter Prompt**: Type your prompt in the text area
2. **Click "Sequential Analysis"**: Start the intelligent processing
3. **Watch Live Steps**: See each router decision and action execution
4. **Review Results**: View validation, extracted context, and improvements
5. **Use Improved Prompt**: Click to replace your prompt with the enhanced version

### **NEW: Persistent Features** ğŸ’¾

#### **Automatic Context Saving**

- All extracted context is **automatically saved** to localStorage
- Context **accumulates over time** - each prompt adds to your profile
- Tech stack, preferences, and goals build up naturally
- Used for better prompt improvements in future

#### **Prompt History** ğŸ“œ

- Every prompt is **automatically saved with timestamp**
- Access via **"ğŸ“œ History"** button in header
- **Search** prompts by content or tags
- **Filter** by specific tags
- **Reuse** any past prompt with one click
- **Export** your entire history as JSON
- See full analysis results for each prompt

#### **History Panel Features**

- ğŸ” Real-time search across all prompts
- ğŸ·ï¸ Tag filtering for organization
- ğŸ“Š Statistics (total prompts, validated, improved)
- ğŸ§© Expandable entries with full details
- ğŸ§  Router decision timeline for each prompt
- âœ¨ One-click prompt reuse
- ğŸ’¾ Export/import functionality
- ğŸ—‘ï¸ Delete individual or clear all

### Example Prompts

**Simple Validation:**

```
Analyze this code and identify bugs. Be thorough and explain your reasoning.
```

**With Context:**

```
I'm a Python developer working on a FastAPI backend. Help me implement
JWT authentication with proper error handling and security best practices.
```

**Complex Task:**

```
I work with React, TypeScript, and Tailwind. I'm building a real-time
chat application. Create a component for message display with typing
indicators, read receipts, and emoji support.
```

## âš™ï¸ Configuration

### AI Provider Settings

Supported providers:

- **Gemini** (Google): Fast, reliable, generous free tier
- **OpenAI**: GPT-4o, GPT-4o-mini
- **Groq**: Ultra-fast Llama models
- **Custom**: Any OpenAI-compatible API

### Router Configuration

```typescript
// In llmRouter.ts
maxIterations = 15; // Safety limit for loops
```

### Extraction Specificity

Each extraction function is focused and specific:

- Personal info: name, location, goals, interests
- Professional: job, tech stack, domain, projects
- Task: current work, specific tasks
- External: tools, frameworks, APIs, libraries
- Tone: style preferences, verbosity

## ğŸ¨ UI Features

### Live Steps Display

- Real-time progress visualization
- Each step shows:
  - Step number and action name
  - Router's reasoning
  - Success/error status
  - Progress estimate

### Modern Design

- Dark glassmorphic theme
- Smooth animations
- Icon-based navigation
- Color-coded results
- Responsive layout

### Results Presentation

- Validation scores with visual indicators
- Context cards with categorized info
- Improved prompt with diff highlighting
- Reasoning and improvements breakdown

## ğŸ”„ Sequential vs Parallel

### Old Approach (Removed)

```
âŒ One big decision â†’ Execute all â†’ Done
   - All-or-nothing
   - No adaptation
   - Wastes API calls
```

### New Approach (Current)

```
âœ… Decide â†’ Execute â†’ Decide â†’ Execute â†’ ...
   - Step-by-step intelligence
   - Adapts to content
   - Only runs what's needed
```

## ğŸš€ Performance

### API Efficiency

- **Typical prompts**: 5-8 LLM calls
- **Simple prompts**: 2-3 calls
- **Complex prompts**: 8-12 calls
- **Maximum**: 16 calls (safety limit)

### Execution Time

- **Simple**: ~5-10 seconds
- **Typical**: ~15-25 seconds
- **Complex**: ~30-45 seconds

### Cost Optimization

- Router calls are lightweight (small tokens)
- Only extracts relevant information
- Skips unnecessary steps
- Accumulates context efficiently

## ğŸ› Debugging

### Enable Console Logs

Router and orchestrator log decisions and results to console.

### Common Issues

**Router loops?**

- Check max iteration limit
- Verify "done" action is being returned
- Review router prompt clarity

**Missing extractions?**

- Router decides based on content
- Not all prompts need all extractions
- Check router's reasoning in UI

**Slow performance?**

- Too many sequential calls
- Consider adjusting router prompt
- Check API response times

## ğŸ“š Documentation

- **[SEQUENTIAL_ROUTER.md](./SEQUENTIAL_ROUTER.md)** - Complete router architecture
- **[PERSISTENCE_FEATURES.md](./PERSISTENCE_FEATURES.md)** - Context & history persistence guide â­ NEW
- **[ARCHITECTURE.md](./ARCHITECTURE.md)** - System design deep dive
- **[CONTEXT_FEATURE.md](./CONTEXT_FEATURE.md)** - Context extraction guide
- **[GETTING_STARTED.md](./GETTING_STARTED.md)** - Step-by-step setup

## ğŸ”’ Privacy

- No data sent anywhere except your chosen LLM provider
- API keys stored locally (Chrome storage or localStorage)
- No tracking, analytics, or telemetry
- All processing happens client-side
- Open source and auditable

## ğŸ’¡ Tips

1. **Be specific in prompts** - More context = better routing decisions
2. **Watch the steps** - Learn how the router thinks
3. **Use improved prompts** - They incorporate all your context
4. **Build context over time** - System remembers your preferences
5. **Clear context when needed** - Start fresh for different projects

## ğŸ¤ Contributing

Ideas to extend:

- Add more extraction categories
- Improve router decision-making
- Add prompt templates
- Export/import functionality
- Context visualization dashboard
- Multi-language support

## ğŸ“„ License

MIT License - free to use and modify

## ğŸ™ Credits

- Architecture inspired by **base-truths** project
- Sequential routing concept for intelligent processing
- Powered by Google Gemini API (and compatible APIs)
- Built with React + Vite + Tailwind CSS

---

**An intelligent prompt analysis system that thinks for itself** ğŸ§ âœ¨

For detailed technical documentation, see [SEQUENTIAL_ROUTER.md](./SEQUENTIAL_ROUTER.md)
