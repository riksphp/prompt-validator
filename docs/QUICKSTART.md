# Prompt Validator - Quick Start Guide

## âœ… What Was Created

A **minimal Chrome Extension and Web App** that validates prompts using LLM, based on the architecture patterns from your **base-truths** project.

### Key Features

- âœ“ Single input box for prompts
- âœ“ LLM validation via Gemini API
- âœ“ Structured JSON output display
- âœ“ Works as Chrome Extension OR Web App
- âœ“ Minimal architecture (only 6 core files!)

---

## ğŸš€ Quick Setup (3 Steps)

### Step 1: Get Your API Key

1. Visit: https://makersuite.google.com/app/apikey
2. Create a Google Gemini API key
3. Copy it (you'll need it in Step 3)

### Step 2: Load the Extension

**Option A: Chrome Extension** (Recommended)

```bash
# Already built! Just load it:
# 1. Open chrome://extensions
# 2. Enable "Developer mode"
# 3. Click "Load unpacked"
# 4. Select: /Users/rishikesh.kumar/Desktop/EAGV2/prompt-validator/dist
# 5. Pin the extension to your toolbar
```

**Option B: Web App**

```bash
cd /Users/rishikesh.kumar/Desktop/EAGV2/prompt-validator
npm run dev
# Opens at http://localhost:5173
```

### Step 3: Configure API Key

1. Click the extension icon (or open web app)
2. Click "âš™ï¸ Settings" button (top-right)
3. Paste your Gemini API key
4. Click "Save"

**You're ready!** ğŸ‰

---

## ğŸ“ How to Use

### Basic Usage

1. **Enter a Prompt**

   ```
   Type or paste any prompt in the text area
   ```

2. **Click "Validate Prompt"**

   ```
   The LLM will analyze your prompt (takes 2-5 seconds)
   ```

3. **View Results**

   ```
   See validation results with boolean flags:
   âœ“ Explicit Reasoning: Yes
   âœ— Tool Separation: No
   ... and more

   Plus an overall clarity assessment
   ```

4. **Validate Another**
   ```
   Click "Validate Another Prompt" to start over
   ```

### Example Prompt

Try this sample prompt:

```
You are an expert code reviewer. Analyze the following code and:

1. Identify any bugs or security issues
2. Suggest performance improvements
3. Provide refactored code with comments
4. Explain your reasoning step by step

If you need clarification on any part, ask before proceeding.

[CODE WOULD GO HERE]
```

**Expected Results:**

- âœ“ Explicit Reasoning: Yes (requests step-by-step explanation)
- âœ“ Structured Output: Yes (numbered list format)
- âœ— Tool Separation: No (no tools mentioned)
- âœ“ Conversation Loop: Yes (asks for clarification)
- âœ“ Instructional Framing: Yes (clear instructions)
- âœ— Internal Self-Checks: No (no self-verification)
- âœ“ Reasoning Type Awareness: Yes (specifies analytical approach)
- âœ“ Fallbacks: Yes (handles ambiguity)

---

## ğŸ¯ Validation Output Format

The LLM returns this exact JSON structure:

```json
{
  "explicit_reasoning": true,
  "structured_output": true,
  "tool_separation": false,
  "conversation_loop": true,
  "instructional_framing": true,
  "internal_self_checks": false,
  "reasoning_type_awareness": true,
  "fallbacks": true,
  "overall_clarity": "Excellent structure with clear instructions and fallback handling."
}
```

---

## ğŸ“ Project Structure

```
prompt-validator/
â”œâ”€â”€ dist/                    â† Built files (load this in Chrome)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ Home.tsx        â† Main UI (input, results, settings)
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ llmClient.ts    â† LLM API integration
â”‚   â”œâ”€â”€ types/
â”‚   â”‚   â””â”€â”€ index.ts        â† TypeScript types
â”‚   â”œâ”€â”€ main.tsx            â† Entry point
â”‚   â””â”€â”€ styles.css          â† Tailwind CSS
â”œâ”€â”€ public/
â”‚   â””â”€â”€ manifest.json       â† Chrome Extension config
â”œâ”€â”€ README.md               â† Full documentation
â”œâ”€â”€ ARCHITECTURE.md         â† Technical architecture details
â””â”€â”€ QUICKSTART.md           â† This file!
```

---

## ğŸ”§ Development Commands

```bash
# Install dependencies (already done)
npm install

# Development mode (web app with hot reload)
npm run dev

# Build for production
npm run build

# Watch mode (rebuild on changes)
npm run build:watch

# Preview production build
npm run preview

# Lint code
npm run lint
```

---

## ğŸ—ï¸ Architecture Highlights

### Inspired by Base-Truths

âœ… **Same patterns, minimal implementation**

| Component        | Description                                    |
| ---------------- | ---------------------------------------------- |
| **Home.tsx**     | Single-page UI with input, validation, results |
| **llmClient.ts** | LLM integration (Gemini API)                   |
| **Storage**      | Chrome Storage + localStorage fallback         |
| **Types**        | TypeScript interfaces for type safety          |
| **Build**        | Vite + React + Tailwind CSS                    |

### Key Differences from Base-Truths

- âŒ No routing (single page)
- âŒ No multi-agent system
- âŒ No dashboard/chat/scoring
- âœ… Focused on ONE task: prompt validation
- âœ… ~6 core files vs 30+ in base-truths
- âœ… Easier to understand and extend

---

## ğŸ¨ How It Works

### Step-by-Step Flow

```
1. User enters prompt
   â†“
2. Click "Validate Prompt"
   â†“
3. llmClient.validatePrompt() called
   â†“
4. Constructs validation prompt for LLM
   â†“
5. Sends to Gemini API
   â†“
6. Receives JSON response
   â†“
7. Parses and validates structure
   â†“
8. Displays results in UI
```

### LLM Validation Prompt Template

The app sends this to Gemini:

```
Analyze the following prompt and validate it according to these criteria.
Return ONLY a JSON object with these exact fields (no additional text):

{
  "explicit_reasoning": boolean,
  "structured_output": boolean,
  "tool_separation": boolean,
  "conversation_loop": boolean,
  "instructional_framing": boolean,
  "internal_self_checks": boolean,
  "reasoning_type_awareness": boolean,
  "fallbacks": boolean,
  "overall_clarity": string
}

Prompt to analyze:
"""
[YOUR PROMPT HERE]
"""

Return ONLY the JSON object, no other text.
```

---

## ğŸ” Privacy & Security

- âœ“ API key stored locally (Chrome Storage or localStorage)
- âœ“ No data sent to external servers (except your configured LLM)
- âœ“ No tracking or analytics
- âœ“ Open source and transparent
- âœ“ Minimal permissions required

---

## ğŸ› Troubleshooting

### "API key not configured"

â†’ Click Settings and enter your Gemini API key

### "API error: 400"

â†’ Check if your API key is valid and active

### "No response from AI"

â†’ Verify API URL in settings (default should work)

### Extension not loading

â†’ Make sure you selected the `dist/` folder, not the root

### Changes not showing

â†’ Click reload icon in chrome://extensions after rebuilding

---

## ğŸš€ Next Steps

### Test It Out

1. Load the extension
2. Configure your API key
3. Try the example prompt above
4. Experiment with different prompts

### Customize It

- Add more validation criteria
- Change the UI styling
- Support different LLM providers
- Add export/import functionality

### Extend It

- See `ARCHITECTURE.md` for extension points
- All code is well-commented
- Follow base-truths patterns

---

## ğŸ“š Documentation

- **README.md** - User guide and features
- **ARCHITECTURE.md** - Technical deep dive
- **QUICKSTART.md** - This file (quick setup)

---

## âœ¨ Summary

You now have a **working Chrome Extension** that:

- âœ… Validates prompts using LLM
- âœ… Returns structured JSON analysis
- âœ… Works as extension or web app
- âœ… Uses base-truths architecture
- âœ… Is minimal and easy to understand

**Total build time**: Already done! Just load and configure.

---

**Happy prompt validating!** ğŸ‰

For questions or issues, refer to:

- `README.md` for detailed usage
- `ARCHITECTURE.md` for technical details
- Source code (it's minimal and well-commented!)
